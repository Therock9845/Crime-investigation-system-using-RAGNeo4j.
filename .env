# LLM Provider Configuration
LLM_PROVIDER=openai

# OpenAI API Configuration
OPENAI_API_KEY="your openai api key here"
OPENAI_BASE_URL= https://api.openai.com/v1(paid) or https://openrouter.ai/api/v1(free trial)  # Optional: custom endpoint
OPENAI_MODEL= # Model to use
OPENAI_PROXY=  # Optional: proxy URL



# Ollama Configuration (if using Ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD="your neo4j local password here"

# Document Processing Configuration
CHUNK_SIZE=400
CHUNK_OVERLAP=50

# Entity Extraction Configuration (NEW)
ENABLE_ENTITY_EXTRACTION=true  # Enable/disable entity extraction
LLM_CONCURRENCY=1  # Concurrent LLM requests for entity extraction
EMBEDDING_CONCURRENCY=1  # Concurrent embedding requests

# Retrieval Configuration (NEW)
MIN_RETRIEVAL_SIMILARITY=0.1  # Minimum similarity threshold
HYBRID_CHUNK_WEIGHT=0.6  # Weight for chunk-based results in hybrid mode
ENABLE_GRAPH_EXPANSION=true  # Enable graph traversal for context expansion

# Graph Expansion Limits (NEW)
MAX_EXPANDED_CHUNKS=500  # Maximum chunks after graph expansion
MAX_ENTITY_CONNECTIONS=20  # Maximum entity connections to follow
MAX_CHUNK_CONNECTIONS=10  # Maximum chunk similarity connections
EXPANSION_SIMILARITY_THRESHOLD=0.1  # Minimum similarity for expansion
MAX_EXPANSION_DEPTH=2  # Maximum depth for graph traversal

# Application Configuration
LOG_LEVEL=INFO

EMBEDDING_CONCURRENCY=3
