# Environment Configuration Template
# Copy this file to .env and update with your actual values

# LLM / Ollama Configuration
# Set provider to 'ollama' to use a local Ollama server for embeddings and LLM
# Supported values: 'openai' (default) or 'ollama'
LLM_PROVIDER=ollama

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4
OPENAI_PROXY=  # Optional: Set if you need to use a proxy

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password_here

# If using Ollama, set the base URL (e.g. http://host.docker.internal:11434 or http://ollama:11434)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
OLLAMA_API_KEY=   # Not required for local Ollama server

# Embedding Configuration
EMBEDDING_MODEL=text-embedding-ada-002
EMBEDDING_CONCURRENCY=3

# Document Processing Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Application Configuration
LOG_LEVEL=INFO
MAX_UPLOAD_SIZE=104857600